{"main_page": "https://arxiv.org/abs/2510.17886", "pdf": "https://arxiv.org/pdf/2510.17886", "title": "Title:\n          Graphical model for tensor factorization by sparse sampling", "authors": "Angelo Giorgio, Riki Nagasawa, Shuta Yokoi, Tomoyuki Obuchi, Hajime Yoshino", "subjects": "Machine Learning (stat.ML); Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Machine Learning (cs.LG)", "abstract": "We consider tensor factorizations based on sparse measurements of the tensor components. The measurements are designed in a way that the underlying graph of interactions is a random graph. The setup will be useful in cases where a substantial amount of data is missing, as in recommendation systems heavily used in social network services. In order to obtain theoretical insights on the setup, we consider statistical inference of the tensor factorization in a high dimensional limit, which we call as dense limit, where the graphs are large and dense but not fully connected. We build message-passing algorithms and test them in a Bayes optimal teacher-student setting. We also develop a replica theory, which becomes exact in the dense limit,to examine the performance of statistical inference."}
{"main_page": "https://arxiv.org/abs/2510.17903", "pdf": "https://arxiv.org/pdf/2510.17903", "title": "Title:\n          Learning Time-Varying Graphs from Incomplete Graph Signals", "authors": "Chuansen Peng, Xiaojing Shen", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "This paper tackles the challenging problem of jointly inferring time-varying network topologies and imputing missing data from partially observed graph signals. We propose a unified non-convex optimization framework to simultaneously recover a sequence of graph Laplacian matrices while reconstructing the unobserved signal entries. Unlike conventional decoupled methods, our integrated approach facilitates a bidirectional flow of information between the graph and signal domains, yielding superior robustness, particularly in high missing-data regimes. To capture realistic network dynamics, we introduce a fused-lasso type regularizer on the sequence of Laplacians. This penalty promotes temporal smoothness by penalizing large successive changes, thereby preventing spurious variations induced by noise while still permitting gradual topological evolution. For solving the joint optimization problem, we develop an efficient Alternating Direction Method of Multipliers (ADMM) algorithm, which leverages the problem's structure to yield closed-form solutions for both the graph and signal subproblems. This design ensures scalability to large-scale networks and long time horizons. On the theoretical front, despite the inherent non-convexity, we establish a convergence guarantee, proving that the proposed ADMM scheme converges to a stationary point. Furthermore, we derive non-asymptotic statistical guarantees, providing high-probability error bounds for the graph estimator as a function of sample size, signal smoothness, and the intrinsic temporal variability of the graph. Extensive numerical experiments validate the approach, demonstrating that it significantly outperforms state-of-the-art baselines in both convergence speed and the joint accuracy of graph learning and signal recovery."}
{"main_page": "https://arxiv.org/abs/2510.17946", "pdf": "https://arxiv.org/pdf/2510.17946", "title": "Title:\n          Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling", "authors": "Sanjan C. Muchandimath, Joaquim R. R. A. Martins, Alex A. Gorodetsky", "subjects": "Methodology (stat.ME); Applications (stat.AP)", "abstract": "Mathematical models in computational physics contain uncertain parameters that impact prediction accuracy. In turbulence modeling, this challenge is especially significant: Reynolds averaged Navier-Stokes (RANS) models, such as the Spalart-Allmaras (SA) model, are widely used for their speed and robustness but often suffer from inaccuracies and associated uncertainties due to imperfect model parameters. Reliable quantification of these uncertainties is becoming increasingly important in aircraft certification by analysis, where predictive credibility is critical. Bayesian inference provides a framework to estimate these parameters and quantify output uncertainty, but traditional methods are prohibitively expensive, especially when relying on high-fidelity simulations. We address the challenge of expensive Bayesian parameter estimation by developing a multi-fidelity framework that combines Markov chain Monte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to efficiently solve inverse problems. The MLMC approach requires correlated samples across different fidelity levels, achieved through a novel transport map-based coupling algorithm. We demonstrate a 50% reduction in inference cost compared to traditional single-fidelity methods on the challenging NACA0012 airfoil at high angles of attack near stall, while delivering realistic uncertainty bounds for model predictions in complex separated flow regimes. These results demonstrate that multi-fidelity approaches significantly improve turbulence parameter calibration, paving the way for more accurate and efficient aircraft certification by analysis."}
{"main_page": "https://arxiv.org/abs/2510.17994", "pdf": "https://arxiv.org/pdf/2510.17994", "title": "Title:\n          Assessing Monotone Dependence: Area Under the Curve Meets Rank Correlation", "authors": "Eva-Maria Walz, Andreas Eberl, Tilmann Gneiting", "subjects": "Methodology (stat.ME)", "abstract": "The assessment of monotone dependence between random variables $X$ and $Y$ is a classical problem in statistics and a gamut of application domains. Consequently, researchers have sought measures of association that are invariant under strictly increasing transformations of the margins, with the extant literature being splintered. Rank correlation coefficients, such as Spearman's Rho and Kendall's Tau, have been studied at great length in the statistical literature, mostly under the assumption that $X$ and $Y$ are continuous. In the case of a dichotomous outcome $Y$, receiver operating characteristic analysis and the asymmetric area under the curve (AUC) measure are used to assess monotone dependence of $Y$ on a covariate $X$. Here we unify and extend thus far disconnected strands of literature, by developing common population level theory, estimators, and tests that bridge continuous and dichotomous settings and apply to all linearly ordered outcomes. In particular, we introduce asymmetric grade correlation, AGC$(X,Y)$, as the covariance of the mid distribution function transforms, or grades, of $X$ and $Y$, divided by the variance of the grade of $Y$. The coefficient of monotone association then is CMA$(X,Y) = \\frac{1}{2} ($AGC$(X,Y) + 1)$. When $X$ and $Y$ are continuous, AGC is symmetric and equals Spearman's Rho. When $Y$ is dichotomous, CMA equals AUC. We establish central limit theorems for the sample versions of AGC and CMA and develop a test of DeLong type for the equality of AGC or CMA values with a shared outcome $Y$. In case studies, we apply the new measures to assess progress in data-driven weather prediction, and to evaluate methods of uncertainty quantification for large language models."}
{"main_page": "https://arxiv.org/abs/2510.18067", "pdf": "https://arxiv.org/pdf/2510.18067", "title": "Title:\n          Principled Argo Modeling using Vecchia-based Gaussian Processes", "authors": "Nian Liu, Jian Cao", "subjects": "Computation (stat.CO); Methodology (stat.ME)", "abstract": "Argo is an international program that collects temperature and salinity observations in the upper two kilometers of the global ocean. Most existing approaches for modeling Argo temperature rely on spatial partitioning, where data are locally modeled by first estimating a prescribed mean structure and then fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such strategies introduce challenges in designing suitable mean structures and defining domain partitions, often resulting in ad hoc modeling choices. In this work, we propose a one-stop Gaussian process regression framework with a generic spatio-temporal covariance function to jointly model Argo temperature data across broad spatial domains. Our fully data-driven approach achieves superior predictive performance compared with methods that require domain partitioning or parametric regression. To ensure scalability over large spatial regions, we employ the Vecchia approximation, which reduces the computational complexity from cubic to quasi-linear in the number of observations while preserving predictive accuracy. Using Argo data from January to March over the years 2007-2016, the same dataset used in prior benchmark studies, we demonstrate that our approach provides a principled, scalable, and interpretable tool for large-scale oceanographic analysis."}
{"main_page": "https://arxiv.org/abs/2510.18068", "pdf": "https://arxiv.org/pdf/2510.18068", "title": "Title:\n          Cartesian Statistics on Spheres", "authors": "Rudolf Beran", "subjects": "Methodology (stat.ME)", "abstract": "Directional data consists of unit vectors in q-dimensions that can be described in polar or Cartesian coordinates. Axial data can be viewed as a pair of directions pointed in opposite directions or as a projection matrix of rank 1. Historically, their statistical analysis has largely been based on a few low-order exponential family models of distributions for random directions or axes. A lack of tractable algebraic forms for the normalizing constants has hindered the use of higher-order exponential families for less constrained modeling. Of interest are functionals of the unknown distribution of the directional/axial data, such as the directional/axial mean, dispersion, or distribution itself. This paper outlines nonparametric estimators and bootstrap confidence sets for such functionals. The procedures are based on the empirical distribution of the directional/axial sample expressed in Cartesian coordinates. Sketched as well are nonparametric comparisons among multiple mean directions or axes, estimation of trend in mean directions, and analysis of q-dimensional observations restricted to lie in a specified compact subset."}
{"main_page": "https://arxiv.org/abs/2510.18071", "pdf": "https://arxiv.org/pdf/2510.18071", "title": "Title:\n          Arbitrated Indirect Treatment Comparisons", "authors": "Yixin Fang, Weili He", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)", "abstract": "Matching-adjusted indirect comparison (MAIC) has been increasingly employed in health technology assessments (HTA). By reweighting subjects from a trial with individual participant data (IPD) to match the covariate summary statistics of another trial with only aggregate data (AgD), MAIC facilitates the estimation of a treatment effect defined with respect to the AgD trial population. This manuscript introduces a new class of methods, termed arbitrated indirect treatment comparisons, designed to address the ``MAIC paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox arises when different sponsors, analyzing the same data, reach conflicting conclusions regarding which treatment is more effective. The underlying issue is that each sponsor implicitly targets a different population. To resolve this inconsistency, the proposed methods focus on estimating treatment effects in a common target population, specifically chosen to be the overlap population."}
{"main_page": "https://arxiv.org/abs/2510.18099", "pdf": "https://arxiv.org/pdf/2510.18099", "title": "Title:\n          Adaptive Grid-Based Thompson Sampling for Efficient Trajectory Discovery", "authors": "Arindam Fadikar, Abby Stevens, Mickael Binois, Nicholson Collier, Jonathan Ozik", "subjects": "Methodology (stat.ME)", "abstract": "Bayesian optimization (BO) is a powerful framework for estimating parameters of computationally expensive simulation models, particularly in settings where the likelihood is intractable and evaluations are costly. In stochastic models every simulation is run with a specific parameter set and an implicit or explicit random seed, where each parameter set and random seed combination generates an individual realization, or trajectory, sampled from an underlying random process. Existing BO approaches typically rely on summary statistics over the realizations, such as means, medians, or quantiles, potentially limiting their effectiveness when trajectory-level information is desired. We propose a trajectory-oriented Bayesian optimization method that incorporates a Gaussian process (GP) surrogate using both input parameters and random seeds as inputs, enabling direct inference at the trajectory level. Using a common random number (CRN) approach, we define a surrogate-based likelihood over trajectories and introduce an adaptive Thompson Sampling algorithm that refines a fixed-size input grid through likelihood-based filtering and Metropolis-Hastings-based densification. This approach concentrates computation on statistically promising regions of the input space while balancing exploration and exploitation. We apply the method to stochastic epidemic models, a simple compartmental and a more computationally demanding agent-based model, demonstrating improved sampling efficiency and faster identification of data-consistent trajectories relative to parameter-only inference."}
{"main_page": "https://arxiv.org/abs/2510.18115", "pdf": "https://arxiv.org/pdf/2510.18115", "title": "Title:\n          Copula Structural Equation Models for Mediation Pathway Analysis", "authors": "Canyi Chen, Ritoban Kundu, Wei Hao, Peter X.-K. Song", "subjects": "Methodology (stat.ME)", "abstract": "Structural equation models (SEMs) are fundamental to causal mediation pathway discovery. However, traditional SEM approaches often rely on \\emph{ad hoc} model specifications when handling complex data structures such as mixed data types or non-normal data in which Gaussian assumptions for errors are rather restrictive. The invocation of copula dependence modeling methods to extend the classical linear SEMs mitigates several of key technical limitations, offering greater modeling flexibility to analyze non-Gaussian data. This paper presents a selective review of major developments in this area, highlighting recent advancements and their methodological implications."}
{"main_page": "https://arxiv.org/abs/2510.18120", "pdf": "https://arxiv.org/pdf/2510.18120", "title": "Title:\n          Generalization Below the Edge of Stability: The Role of Data Geometry", "authors": "Tongtong Liang, Alexander Cloninger, Rahul Parhi, Yu-Xiang Wang", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Understanding generalization in overparameterized neural networks hinges on the interplay between the data geometry, neural architecture, and training dynamics. In this paper, we theoretically explore how data geometry controls this implicit bias. This paper presents theoretical results for overparameterized two-layer ReLU networks trained below the edge of stability. First, for data distributions supported on a mixture of low-dimensional balls, we derive generalization bounds that provably adapt to the intrinsic dimension. Second, for a family of isotropic distributions that vary in how strongly probability mass concentrates toward the unit sphere, we derive a spectrum of bounds showing that rates deteriorate as the mass concentrates toward the sphere. These results instantiate a unifying principle: When the data is harder to \"shatter\" with respect to the activation thresholds of the ReLU neurons, gradient descent tends to learn representations that capture shared patterns and thus finds solutions that generalize well. On the other hand, for data that is easily shattered (e.g., data supported on the sphere) gradient descent favors memorization. Our theoretical results consolidate disparate empirical findings that have appeared in the literature."}
{"main_page": "https://arxiv.org/abs/2510.18126", "pdf": "https://arxiv.org/pdf/2510.18126", "title": "Title:\n          On A Necessary Condition For Posterior Inconsistency: New Insights From A Classic Counterexample", "authors": "Nicola Bariletto, Stephen G. Walker", "subjects": "Statistics Theory (math.ST)", "abstract": "The consistency of posterior distributions in density estimation is at the core of Bayesian statistical theory. Classical work established sufficient conditions, typically combining KL support with complexity bounds on sieves of high prior mass, to guarantee consistency with respect to the Hellinger distance. Yet no systematic theory explains a widely held belief: under KL support, Hellinger consistency is exceptionally hard to violate. This suggests that existing sufficient conditions, while useful in practice, may overlook some key aspects of posterior behavior. We address this gap by directly investigating what must fail for inconsistency to arise, aiming to identify a substantive necessary condition for Hellinger inconsistency. Our starting point is Andrew Barron's classical counterexample, the only known violation of Hellinger consistency under KL support, which relies on a contrived family of oscillatory densities and a prior with atoms. We show that, within a broad class of models including Barron's, inconsistency requires persistent posterior concentration on densities with exponentially high likelihood ratios. In turn, such behavior demands a prior encoding implausibly precise knowledge of the true, yet unknown data-generating distribution, making inconsistency essentially unattainable in any realistic inference problem. Our results confirm the long-standing intuition that posterior inconsistency in density estimation is not a natural phenomenon, but rather an artifact of pathological prior constructions."}
{"main_page": "https://arxiv.org/abs/2510.18149", "pdf": "https://arxiv.org/pdf/2510.18149", "title": "Title:\n          Conformal Inference For Missing Data under Multiple Robust Learning", "authors": "Wenlu Tang, Hongni Wang, Xingcai Zhou, Bei Jiang, Linglong Kong", "subjects": "Methodology (stat.ME)", "abstract": "We develop a novel approach to tackle the common but challenging problem of conformal inference for missing data in machine learning, focusing on Missing at Random (MAR) data. We propose a new procedure Conformal prediction for Missing data under Multiple Robust Learning (CM--MRL) that combines split conformal calibration with a multiple robust empirical-likelihood (EL) reweighting scheme. The method proceeds via a double calibration by reweighting the complete-case scores by EL so that their distribution matches the full calibration distribution implied by MAR, even when some working models are misspecified. We demonstrate the asymptotic behavior of our estimators through empirical process theory and provide reliable coverage for our prediction intervals, both marginally and conditionally and we further show an interval-length dominance result. We show the effectiveness of the proposed method by several numerical experiments in the presence of missing data."}
{"main_page": "https://arxiv.org/abs/2510.18161", "pdf": "https://arxiv.org/pdf/2510.18161", "title": "Title:\n          Beating the Winner's Curse via Inference-Aware Policy Optimization", "authors": "Hamsa Bastani, Osbert Bastani, Bryce McLaughlin", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Econometrics (econ.EM)", "abstract": "There has been a surge of recent interest in automatically learning policies to target treatment decisions based on rich individual covariates. A common approach is to train a machine learning model to predict counterfactual outcomes, and then select the policy that optimizes the predicted objective value. In addition, practitioners also want confidence that the learned policy has better performance than the incumbent policy according to downstream policy evaluation. However, due to the winner's curse-an issue where the policy optimization procedure exploits prediction errors rather than finding actual improvements-predicted performance improvements are often not substantiated by downstream policy optimization. To address this challenge, we propose a novel strategy called inference-aware policy optimization, which modifies policy optimization to account for how the policy will be evaluated downstream. Specifically, it optimizes not only for the estimated objective value, but also for the chances that the policy will be statistically significantly better than the observational policy used to collect data. We mathematically characterize the Pareto frontier of policies according to the tradeoff of these two goals. Based on our characterization, we design a policy optimization algorithm that uses machine learning to predict counterfactual outcomes, and then plugs in these predictions to estimate the Pareto frontier; then, the decision-maker can select the policy that optimizes their desired tradeoff, after which policy evaluation can be performed on the test set as usual. Finally, we perform simulations to illustrate the effectiveness of our methodology."}
{"main_page": "https://arxiv.org/abs/2510.18215", "pdf": "https://arxiv.org/pdf/2510.18215", "title": "Title:\n          The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective", "authors": "Haixiang Lan, Luofeng Liao, Adam N. Elmachtoub, Christian Kroer, Henry Lam, Haofeng Zhang", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "Data-driven stochastic optimization is ubiquitous in machine learning and operational decision-making problems. Sample average approximation (SAA) and model-based approaches such as estimate-then-optimize (ETO) or integrated estimation-optimization (IEO) are all popular, with model-based approaches being able to circumvent some of the issues with SAA in complex context-dependent problems. Yet the relative performance of these methods is poorly understood, with most results confined to the dichotomous cases of the model-based approach being either well-specified or misspecified. We develop the first results that allow for a more granular analysis of the relative performance of these methods under a local misspecification setting, which models the scenario where the model-based approach is nearly well-specified. By leveraging tools from contiguity theory in statistics, we show that there is a bias-variance tradeoff between SAA, IEO, and ETO under local misspecification, and that the relative importance of the bias and the variance depends on the degree of local misspecification. Moreover, we derive explicit expressions for the decision bias, which allows us to characterize (un)impactful misspecification directions, and provide further geometric understanding of the variance."}
{"main_page": "https://arxiv.org/abs/2510.18241", "pdf": "https://arxiv.org/pdf/2510.18241", "title": "Title:\n          Non-Parametric Estimation Techniques of Factor Copula Model using Proxies", "authors": "Bahareh Ghanbari, Pavel Krupskiy, Laleh Tafakori, Yan Wang", "subjects": "Methodology (stat.ME)", "abstract": "Parametric factor copula models typically work well in modeling multivariate dependencies due to their flexibility and ability to capture complex dependency structures. However, accurately estimating the linking copulas within these mod- els remains challenging, especially when working with high-dimensional data. This paper proposes a novel approach for estimating linking copulas based on a non-parametric kernel estimator. Unlike conventional parametric methods, our approach utilizes the flexibility of kernel density estimation to capture the un- derlying dependencies more accurately, particularly in scenarios where the un- derlying copula structure is complex or unknown. We show that the proposed estimator is consistent under mild conditions and demonstrate its effectiveness through extensive simulation studies. Our findings suggest that the proposed approach offers a promising avenue for modeling multivariate dependencies, par- ticularly in applications requiring robust and efficient estimation of copula-based models."}
{"main_page": "https://arxiv.org/abs/2510.18242", "pdf": "https://arxiv.org/pdf/2510.18242", "title": "Title:\n          The Picard-Lagrange Framework for Higher-Order Langevin Monte Carlo", "authors": "Jaideep Mahajan, Kaihong Zhang, Feng Liang, Jingbo Liu", "subjects": "Statistics Theory (math.ST); Methodology (stat.ME); Machine Learning (stat.ML)", "abstract": "Sampling from log-concave distributions is a central problem in statistics and machine learning. Prior work establishes theoretical guarantees for Langevin Monte Carlo algorithm based on overdamped and underdamped Langevin dynamics and, more recently, some third-order variants. In this paper, we introduce a new sampling algorithm built on a general $K$th-order Langevin dynamics, extending beyond second- and third-order methods. To discretize the $K$th-order dynamics, we approximate the drift induced by the potential via Lagrange interpolation and refine the node values at the interpolation points using Picard-iteration corrections, yielding a flexible scheme that fully utilizes the acceleration of higher-order Langevin dynamics. For targets with smooth, strongly log-concave densities, we prove dimension-dependent convergence in Wasserstein distance: the sampler achieves $\\varepsilon$-accuracy within $\\widetilde O(d^{\\frac{K-1}{2K-3}}\\varepsilon^{-\\frac{2}{2K-3}})$ gradient evaluations for $K \\ge 3$. To our best knowledge, this is the first sampling algorithm achieving such query complexity. The rate improves with the order $K$ increases, yielding better rates than existing first to third-order approaches."}
{"main_page": "https://arxiv.org/abs/2510.18247", "pdf": "https://arxiv.org/pdf/2510.18247", "title": "Title:\n          Quantifying Periodicity in Non-Euclidean Random Objects", "authors": "Jiazhen Xu, Andrew T. A. Wood, Tao Zou", "subjects": "Methodology (stat.ME)", "abstract": "Time-varying non-Euclidean random objects are playing a growing role in modern data analysis, and periodicity is a fundamental characteristic of time-varying data. However, quantifying periodicity in general non-Euclidean random objects remains largely unexplored. In this work, we introduce a novel nonparametric framework for quantifying periodicity in random objects within a general metric space that lacks Euclidean structures. Our approach formulates periodicity estimation as a model selection problem and provides methodologies for period estimation, data-driven tuning parameter selection, and periodic component extraction. Our theoretical contributions include establishing the consistency of period estimation without relying on linearity properties used in the literature for Euclidean data, providing theoretical support for data-driven tuning parameter selection, and deriving uniform convergence results for periodic component estimation. Through extensive simulation studies covering three distinct types of time-varying random objects such as compositional data, networks, and functional data, we showcase the superior accuracy achieved by our approach in periodicity quantification. Finally, we apply our method to various real datasets, including U.S. electricity generation compositions, New York City transportation networks, and Germany's water consumption curves, highlighting its practical relevance in identifying and quantifying meaningful periodic patterns."}
{"main_page": "https://arxiv.org/abs/2510.18252", "pdf": "https://arxiv.org/pdf/2510.18252", "title": "Title:\n          Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN", "authors": "Luis H. Chia", "subjects": "Applications (stat.AP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Credit scoring models face a critical challenge: severe class imbalance, with default rates typically below 10%, which hampers model learning and predictive performance. While synthetic data augmentation techniques such as SMOTE and ADASYN have been proposed to address this issue, the optimal augmentation ratio remains unclear, with practitioners often defaulting to full balancing (1:1 ratio) without empirical justification. This study systematically evaluates 10 data augmentation scenarios using the Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x, 3x). All models were trained using XGBoost and evaluated on a held-out test set of 29,173 real observations. Statistical significance was assessed using bootstrap testing with 1,000 iterations. Key findings reveal that ADASYN with 1x multiplication (doubling the minority class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of 0.3557, representing statistically significant improvements of +0.77% and +3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors (2x and 3x) resulted in performance degradation, with 3x showing a -0.48% decrease in AUC, suggesting a \"law of diminishing returns\" for synthetic oversampling. The optimal class imbalance ratio was found to be 6.6:1 (majority:minority), contradicting the common practice of balancing to 1:1. This work provides the first empirical evidence of an optimal \"sweet spot\" for data augmentation in credit scoring, with practical guidelines for industry practitioners and researchers working with imbalanced datasets. While demonstrated on a single representative dataset, the methodology provides a reproducible framework for determining optimal augmentation ratios in other imbalanced domains."}
{"main_page": "https://arxiv.org/abs/2510.18259", "pdf": "https://arxiv.org/pdf/2510.18259", "title": "Title:\n          Learning under Quantization for High-Dimensional Linear Regression", "authors": "Dechen Zhang, Junwei Su, Difan Zou", "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "The use of low-bit quantization has emerged as an indispensable technique for enabling the efficient training of large-scale models. Despite its widespread empirical success, a rigorous theoretical understanding of its impact on learning performance remains notably absent, even in the simplest linear regression setting. We present the first systematic theoretical study of this fundamental question, analyzing finite-step stochastic gradient descent (SGD) for high-dimensional linear regression under a comprehensive range of quantization targets: data, labels, parameters, activations, and gradients. Our novel analytical framework establishes precise algorithm-dependent and data-dependent excess risk bounds that characterize how different quantization affects learning: parameter, activation, and gradient quantization amplify noise during training; data quantization distorts the data spectrum; and data and label quantization introduce additional approximation and quantized error. Crucially, we prove that for multiplicative quantization (with input-dependent quantization step), this spectral distortion can be eliminated, and for additive quantization (with constant quantization step), a beneficial scaling effect with batch size emerges. Furthermore, for common polynomial-decay data spectra, we quantitatively compare the risks of multiplicative and additive quantization, drawing a parallel to the comparison between FP and integer quantization methods. Our theory provides a powerful lens to characterize how quantization shapes the learning dynamics of optimization algorithms, paving the way to further explore learning theory under practical hardware constraints."}
{"main_page": "https://arxiv.org/abs/2510.18290", "pdf": "https://arxiv.org/pdf/2510.18290", "title": "Title:\n          Consistency of Nonparametric Density Estimators in CAT(0) Orthant Space", "authors": "Yuki Takazawa, Tomonari Sei", "subjects": "Statistics Theory (math.ST); Methodology (stat.ME)", "abstract": "The inference of evolutionary histories is a central problem in evolutionary biology. The analysis of a sample of phylogenetic trees can be conducted in Billera-Holmes-Vogtmann tree space, which is a CAT(0) metric space of phylogenetic trees. The globally non-positively curved (CAT(0)) property of this space enables the extension of various statistical techniques. In the problem of nonparametric density estimation, two primary methods, kernel density estimation and log-concave maximum likelihood estimation, have been proposed, yet their theoretical properties remain largely unexplored. In this paper, we address this gap by proving the consistency of these estimators in a more general setting$\\unicode{x2014}$CAT(0) orthant spaces, which include BHV tree space. We extend log-concave approximation techniques to this setting and establish consistency via the continuity of the log-concave projection map. We also modify the kernel density estimator to correct boundary bias and establish uniform consistency using empirical process theory."}
{"main_page": "https://arxiv.org/abs/2510.18332", "pdf": "https://arxiv.org/pdf/2510.18332", "title": "Title:\n          Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning", "authors": "Gargi Roy, Dalia Chakrabarty", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)", "abstract": "We introduce parametrisation of that property of the available training dataset, that necessitates an inhomogeneous correlation structure for the function that is learnt as a model of the relationship between the pair of variables, observations of which comprise the considered training data. We refer to a parametrisation of this property of a given training set, as its ``inhomogeneity parameter''. It is easy to compute this parameter for small-to-large datasets, and we demonstrate such computation on multiple publicly-available datasets, while also demonstrating that conventional ``non-stationarity'' of data does not imply a non-zero inhomogeneity parameter of the dataset. We prove that - within the probabilistic Gaussian Process-based learning approach - a training set with a non-zero inhomogeneity parameter renders it imperative, that the process that is invoked to model the sought function, be non-stationary. Following the learning of a real-world multivariate function with such a Process, quality and reliability of predictions at test inputs, are demonstrated to be affected by the inhomogeneity parameter of the training data."}
{"main_page": "https://arxiv.org/abs/2510.18367", "pdf": "https://arxiv.org/pdf/2510.18367", "title": "Title:\n          Wasserstein projection estimators for circular distributions", "authors": "Naoki Otani, Takeru Matsuda", "subjects": "Statistics Theory (math.ST)", "abstract": "For statistical models on circles, we investigate performance of estimators defined as the projections of the empirical distribution with respect to the Wasserstein distance. We develop algorithms for computing the Wasserstein projection estimators based on a formula of the Wasserstein distances on circles. Numerical results on the von Mises, wrapped Cauchy, and sine-skewed von Mises distributions show that the accuracy of the Wasserstein projection estimators is comparable to the maximum likelihood estimator. In addition, the $L^1$-Wasserstein projection estimator is found to be robust against noise contamination."}
{"main_page": "https://arxiv.org/abs/2510.18503", "pdf": "https://arxiv.org/pdf/2510.18503", "title": "Title:\n          New closed-form estimators for discrete distributions", "authors": "Adrian Fischer", "subjects": "Statistics Theory (math.ST)", "abstract": "We revisit the problem of parameter estimation for discrete probability distributions with values in $\\mathbb{Z}^d$. To this end, we adapt a technique called Stein's Method of Moments to discrete distributions which often gives closed-form estimators when standard methods such as maximum likelihood estimation (MLE) require numerical optimization. These new estimators exhibit good performance in small-sample settings which is demonstrated by means of a comparison to the MLE through simulation studies. We pay special attention to truncated distributions and show that the asymptotic behavior of our estimators is not affected by an unknown (rectangular) truncation domain."}
{"main_page": "https://arxiv.org/abs/2510.18547", "pdf": "https://arxiv.org/pdf/2510.18547", "title": "Title:\n          Hyperparameter Selection via Early Stopping for Bayesian Semilinear PDEs", "authors": "Maia Tienstra, Gottfried Hastermann", "subjects": "Statistics Theory (math.ST)", "abstract": "We study non-linear Bayesian inverse problems arising from semilinear partial differential equations (PDEs) that can be transformed into linear Bayesian inverse problems. We are then able to extend the early stopping for Ensemble Kalman-Bucy Filter (EnKBF) to these types of linearisable nonlinear problems as a way to tune the prior distribution. Using the linearisation method introduced in \\cite{koers2024}, we transform the non-linear problem into a linear one, apply early stopping based on the discrepancy principle, and then pull back the resulting posterior to the posterior for the original parameter of interest. Following \\cite{tienstra2025}, we show that this approach yields adaptive posterior contraction rates and frequentist coverage guarantees, under mild conditions on the prior covariance operator. From this, it immediately follows that Tikhonov regularisation coupled with the discrepancy principle contracts at the same rate. The proposed method thus provides a data-driven way to tune Gaussian priors via early stopping, which is both computationally efficient and statistically near optimal for nonlinear problems. Lastly, we demonstrate our results theoretically and numerically for the classical benchmark problem, the time-independent Schr\u00f6dinger equation."}
{"main_page": "https://arxiv.org/abs/2510.18548", "pdf": "https://arxiv.org/pdf/2510.18548", "title": "Title:\n          Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data", "authors": "Ying Yao, Daniel J. Graham", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)", "abstract": "Accurate annual average daily traffic (AADT) data are vital for transport planning and infrastructure management. However, automatic traffic detectors across national road networks often provide incomplete coverage, leading to underrepresentation of minor roads. While recent machine learning advances have improved AADT estimation at unmeasured locations, most models produce only point predictions and overlook estimation uncertainty. This study addresses that gap by introducing an interval prediction approach that explicitly quantifies predictive uncertainty. We integrate a Quantile Random Forest model with Principal Component Analysis to generate AADT prediction intervals, providing plausible traffic ranges bounded by estimated minima and maxima. Using data from over 2,000 minor roads in England and Wales, and evaluated with specialized interval metrics, the proposed method achieves an interval coverage probability of 88.22%, a normalized average width of 0.23, and a Winkler Score of 7,468.47. By combining machine learning with spatial and high-dimensional analysis, this framework enhances both the accuracy and interpretability of AADT estimation, supporting more robust and informed transport planning."}
{"main_page": "https://arxiv.org/abs/2510.18598", "pdf": "https://arxiv.org/pdf/2510.18598", "title": "Title:\n          Measuring deviations from spherical symmetry", "authors": "Lujia Bai, Holger Dette", "subjects": "Methodology (stat.ME); Statistics Theory (math.ST)", "abstract": "Most of the work on checking spherical symmetry assumptions on the distribution of the $p$-dimensional random vector $Y$ has its focus on statistical tests for the null hypothesis of exact spherical symmetry. In this paper, we take a different point of view and propose a measure for the deviation from spherical symmetry, which is based on the minimum distance between the distribution of the vector $\\big (\\|Y\\|, Y/ \\|Y\\| )^\\top $ and its best approximation by a distribution of a vector $\\big (\\|Y_s\\|, Y_s/ \\|Y_s \\| )^\\top $ corresponding to a random vector $Y_s$ with a spherical distribution. We develop estimators for the minimum distance with corresponding statistical guarantees (provided by asymptotic theory) and demonstrate the applicability of our approach by means of a simulation study and a real data example."}
{"main_page": "https://arxiv.org/abs/2510.18599", "pdf": "https://arxiv.org/pdf/2510.18599", "title": "Title:\n          A new implementation of Network GARCH Model", "authors": "Peiyi Zhou", "subjects": "Methodology (stat.ME)", "abstract": "Volatility clustering and spillovers are key features of real-world financial time series when there are a lot of cross-sectional financial assets. While network analysis helps connect stocks that are 'similar' or 'correlated', which is effective to link volatility spillovers between stocks, contemporary multivariate ARCH-GARCH formulations struggle to represent structured network dependence and remain parsimonious. We introduce the Generalised Network GARCH (GNGARCH) model as a network volatility model that embeds the GARCH dynamics within the Generalised Network Autoregressive (GNAR) framework, to capture the dynamic volatility of financial asset return by both the asset itself and its 'neighbouring' assets from the constructed virtual network. The proposed volatility model GNGARCH also addresses the limitations for current studies of network GARCH by adapting neighbouring volatility persistence, dynamic conditional covariance updates, and allowing higher-order neighbouring effects rather than only immediate neighbours. This paper provides the model derivation, vectorisation and conversion, stationarity conditions, and also an extension by incorporating threshold coefficients to capture leverage effects. We show that the GNGARCH is a valid volatility model satisfying the stylised facts of financial return series through simulation. Parameter estimation is then performed by using squared returns as variance proxy and minimising a loss function that is either mean squared error (MSE) or quasi-likelihood (QLIKE). We apply our model on 75 of the most active US stocks under a virtual network, and highlight the model's ability in volatility estimation and forecast."}
{"main_page": "https://arxiv.org/abs/2510.18639", "pdf": "https://arxiv.org/pdf/2510.18639", "title": "Title:\n          Distributional regression for seasonal data: an application to river flows", "authors": "Samuel Perreault, Silvana M. Pesenti, Daniyal Shahzad", "subjects": "Applications (stat.AP); Risk Management (q-fin.RM); Methodology (stat.ME)", "abstract": "Risk assessment in casualty insurance, such as flood risk, traditionally relies on extreme-value methods that emphasizes rare events. These approaches are well-suited for characterizing tail risk, but do not capture the broader dynamics of environmental variables such as moderate or frequent loss events. To complement these methods, we propose a modelling framework for estimating the full (daily) distribution of environmental variables as a function of time, that is a distributional version of typical climatological summary statistics, thereby incorporating both seasonal variation and gradual long-term changes. Aside from the time trend, to capture seasonal variation our approach simultaneously estimates the distribution for each instant of the seasonal cycle, without explicitly modelling the temporal dependence present in the data. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive Models for Location, Scale, and Shape), where the parameters of the distribution vary over the seasonal cycle as a function of explanatory variables depending only on the time of year, and not on the past values of the process under study. Ignoring the temporal dependence in the seasonal variation greatly simplifies the modelling but poses inference challenges that we clarify and overcome. We apply our framework to daily river flow data from three hydrometric stations along the Fraser River in British Columbia, Canada, and analyse the flood of the Fraser River in early winter of 2021."}
{"main_page": "https://arxiv.org/abs/2510.18654", "pdf": "https://arxiv.org/pdf/2510.18654", "title": "Title:\n          Differentially Private E-Values", "authors": "Daniel Csillag, Diego Mesquita", "subjects": "Methodology (stat.ME); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)", "abstract": "E-values have gained prominence as flexible tools for statistical inference and risk control, enabling anytime- and post-hoc-valid procedures under minimal assumptions. However, many real-world applications fundamentally rely on sensitive data, which can be leaked through e-values. To ensure their safe release, we propose a general framework to transform non-private e-values into differentially private ones. Towards this end, we develop a novel biased multiplicative noise mechanism that ensures our e-values remain statistically valid. We show that our differentially private e-values attain strong statistical power, and are asymptotically as powerful as their non-private counterparts. Experiments across online risk monitoring, private healthcare, and conformal e-prediction demonstrate our approach's effectiveness and illustrate its broad applicability."}
{"main_page": "https://arxiv.org/abs/2510.18777", "pdf": "https://arxiv.org/pdf/2510.18777", "title": "Title:\n          A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models", "authors": "Yen-Chi Chen", "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)", "abstract": "While Variational Inference (VI) is central to modern generative models like Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its pedagogical treatment is split across disciplines. In statistics, VI is typically framed as a Bayesian method for posterior approximation. In machine learning, however, VAEs and DDMs are developed from a Frequentist viewpoint, where VI is used to approximate a maximum likelihood estimator. This creates a barrier for statisticians, as the principles behind VAEs and DDMs are hard to contextualize without a corresponding Frequentist introduction to VI. This paper provides that introduction: we explain the theory for VI, VAEs, and DDMs from a purely Frequentist perspective, starting with the classical Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable solution for intractable E-steps and how VAEs and DDMs are natural, deep-learning-based extensions of this framework, thereby bridging the gap between classical statistical inference and modern generative AI."}
{"main_page": "https://arxiv.org/abs/2510.18818", "pdf": "https://arxiv.org/pdf/2510.18818", "title": "Title:\n          Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad", "authors": "Jay JH Park, Rebecca K. Metcalfe, Nathaniel Dyrkton, Yichen Yan, Shomoita Alam, Kevin Phelan, Ibrahim Sana, Susan Shepherd", "subjects": "Applications (stat.AP); Methodology (stat.ME)", "abstract": "Current practices for designing cluster-randomized trials (cRCTs) typically rely on closed-form formulas for power calculations. For cRCTs using covariate-constrained randomization, the utility of conventional calculations might be limited, particularly when data is nested. We compared simulation-based planning of a nested cRCT using covariate-constrained randomization to conventional power calculations using OptiMAx-Chad as a case study. OptiMAx-Chad will examine the impact of embedding mass distribution of small-quantity lipid-based nutrient supplements within an expanded programme on immunization on first-dose measles-containing vaccine (MCV1) coverage among children aged 12-24 months in rural villages in Ngouri. Within the 12 health areas to be randomized, a random subset of villages will be selected for outcome collection. 1,000,000 assignments of health areas with different possible village selections were generated using covariate-constrained randomization to balance baseline village characteristics. The empirically estimated intracluster correlation coefficient (ICC) and the World Health Organization (WHO) recommended values of 1/3 and 1/6 were considered. The desired operating characteristics were 80% power at 0.05 one-sided type I error rate. Using conventional calculations target power for a realistic treatment effect could not be achieved with the WHO recommended values. Conventional calculations also showed a plateau in power after a certain cluster size. Our simulations matched the design of OptiMAx-Chad with covariate adjustment and random selection, and showed that power did not plateau. Instead, power increased with increasing cluster size. Planning complex cRCTs with covariate constrained randomization and a multi-nested data structure with conventional closed-form formulas can be misleading. Simulations can improve the planning of cRCTs."}
{"main_page": "https://arxiv.org/abs/2510.18834", "pdf": "https://arxiv.org/pdf/2510.18834", "title": "Title:\n          Testing Risk Difference of Two Proportions for Combined Unilateral and Bilateral Data", "authors": "Jia Zhou, Chang-Xing Ma", "subjects": "Methodology (stat.ME)", "abstract": "In clinical studies with paired organs, binary outcomes often exhibit intra-subject correlation and may include a mixture of unilateral and bilateral observations. Under Donner's constant correlation model, we develop three likelihood-based test statistics (the likelihood ratio, Wald-type, and score tests) for assessing the risk difference between two proportions. Simulation studies demonstrate good control of type I error and comparable power among the three tests, with the score test showing slightly better stability. Applications to otolaryngologic and ophthalmologic data illustrate the methods. An online calculator is also provided for power analysis and risk difference testing. The score test is recommended for practical use and future studies with combined unilateral and bilateral binary data."}
{"main_page": "https://arxiv.org/abs/2510.18843", "pdf": "https://arxiv.org/pdf/2510.18843", "title": "Title:\n          Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects", "authors": "Pawel Morzywolek, Peter B. Gilbert, Alex Luedtke", "subjects": "Methodology (stat.ME); Statistics Theory (math.ST); Machine Learning (stat.ML)", "abstract": "We provide an inferential framework to assess variable importance for heterogeneous treatment effects. This assessment is especially useful in high-risk domains such as medicine, where decision makers hesitate to rely on black-box treatment recommendation algorithms. The variable importance measures we consider are local in that they may differ across individuals, while the inference is global in that it tests whether a given variable is important for any individual. Our approach builds on recent developments in semiparametric theory for function-valued parameters, and is valid even when statistical machine learning algorithms are employed to quantify treatment effect heterogeneity. We demonstrate the applicability of our method to infectious disease prevention strategies."}
