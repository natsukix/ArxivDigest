import os
import re
import io
import asyncio
import logging
import tempfile
import requests
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor

from openai import OpenAI
import discord
from discord.ext import commands

# PDF parsing: prefer pypdf (PyPDF2) or pdfplumber if available
try:
    from pypdf import PdfReader
except Exception:
    try:
        from PyPDF2 import PdfReader
    except Exception:
        PdfReader = None

load_dotenv()

LOGGER = logging.getLogger("discord_pdf_bot")
logging.basicConfig(level=logging.INFO)

DISCORD_BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")
DISCORD_FORUM_CHANNEL_ID = int(os.getenv("DISCORD_FORUM_CHANNEL_ID", "0"))  # æ¯æ—¥ã®è«–æ–‡ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚©ãƒ¼ãƒ©ãƒ 
DISCORD_ARXIV_ANALYSIS_CHANNEL_ID = int(os.getenv("DISCORD_ARXIV_ANALYSIS_CHANNEL_ID", "0"))  # åˆ†æçµæœæŠ•ç¨¿å…ˆ
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MAX_TOKENS_PER_CHUNK = int(os.getenv("MAX_TOKENS_PER_CHUNK", "4000"))
REACTION_EMOJI = os.getenv("REACTION_EMOJI", "ğŸ”")  # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯¾è±¡ã®çµµæ–‡å­—
PDF_ANALYSIS_MODEL = os.getenv("PDF_ANALYSIS_MODEL", "gpt-4o-mini")  # .env ã‹ã‚‰èª­ã¿è¾¼ã‚€

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèªãƒ­ã‚°
print(f"\n=== BOT CONFIGURATION ===")
print(f"DISCORD_BOT_TOKEN: {'SET' if DISCORD_BOT_TOKEN else 'NOT SET'}")
print(f"DISCORD_FORUM_CHANNEL_ID: {DISCORD_FORUM_CHANNEL_ID}")
print(f"DISCORD_ARXIV_ANALYSIS_CHANNEL_ID: {DISCORD_ARXIV_ANALYSIS_CHANNEL_ID}")
print(f"OPENAI_API_KEY: {'SET' if OPENAI_API_KEY else 'NOT SET'}")
print(f"REACTION_EMOJI: '{REACTION_EMOJI}'")
print(f"PDF_ANALYSIS_MODEL: {PDF_ANALYSIS_MODEL}")
print(f"========================\n")

intents = discord.Intents.all()  # ã™ã¹ã¦ã®Intentã‚’æœ‰åŠ¹åŒ–
intents.reactions = True  # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’å—ã‘å–ã‚‹ãŸã‚ã«å¿…è¦
bot = commands.Bot(command_prefix="!", intents=intents)

# ThreadPoolExecutor ã§ OpenAI API ã‚’éåŒæœŸå®Ÿè¡Œ
executor = ThreadPoolExecutor(max_workers=2)

ARXIV_PDF_RE = re.compile(r"(?:https?://)?(?:www\.)?arxiv\.org/(?:pdf|abs)/([0-9.]+v?\d*)")

async def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    if PdfReader is None:
        raise RuntimeError("No PDF reader available (install pypdf or PyPDF2)")

    with io.BytesIO(pdf_bytes) as fh:
        reader = PdfReader(fh)
        texts = []
        for page in reader.pages:
            try:
                txt = page.extract_text() or ""
            except Exception:
                txt = ""
            texts.append(txt)
    return "\n".join(texts)


def split_discord_message(text: str, max_length: int = 2000) -> list:
    """
    Discordã®æ–‡å­—åˆ¶é™ï¼ˆ2000æ–‡å­—ï¼‰ã«åˆã‚ã›ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    current_chunk = ""
    
    for line in text.split('\n'):
        if len(current_chunk) + len(line) + 1 > max_length:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line
        else:
            if current_chunk:
                current_chunk += '\n' + line
            else:
                current_chunk = line
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


async def analyze_text_with_openai(text: str, paper_id: str) -> tuple:
    """
    åˆ†æçµæœã‚’æ—¥æœ¬èªã¨è‹±èªã«åˆ†å‰²ã—ã¦è¿”ã™
    Returns: (japanese_analysis, english_analysis)
    """
    prompt = f"""
You are a research assistant. Analyze the following arXiv paper (id: {paper_id}).

Provide analysis in BOTH Japanese and English in the following format:

## æ—¥æœ¬èªã§ã®åˆ†æ
- çŸ­ã„è¦ç´„ï¼ˆ1-2æ–‡ï¼‰
- ä¸»ãªè²¢çŒ®ï¼ˆç®‡æ¡æ›¸ãï¼‰
- ã‚³ã‚¢æŠ€è¡“çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆç°¡æ½”ã«ï¼‰
- ä¸»è¦ãªå®Ÿé¨“çµæœ
- åˆ¶é™äº‹é …ã¨ä»Šå¾Œã®å±•æœ›

## English Analysis
- Short summary (1-2 sentences)
- Main contributions (bullet points)
- Core technical approach (concise)
- Key experimental results
- Limitations and potential next steps

Use Markdown format.

Paper text (first 100k chars):\n
{text[:100000]}
"""
    
    response = client.chat.completions.create(
        model=PDF_ANALYSIS_MODEL,
        messages=[{"role": "user", "content": prompt}],
    )
    content = response.choices[0].message.content
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ—¥æœ¬èªã¨è‹±èªã«åˆ†å‰²
    # "## English Analysis" ã§åˆ†å‰²
    parts = content.split("## English Analysis")
    japanese_analysis = parts[0].replace("## æ—¥æœ¬èªã§ã®åˆ†æ", "").strip()
    english_analysis = parts[1].strip() if len(parts) > 1 else ""
    
    return japanese_analysis, english_analysis


def analyze_text_with_openai_sync(text: str, paper_id: str) -> tuple:
    """åŒæœŸç‰ˆ (ThreadPoolExecutor ã§å®Ÿè¡Œç”¨)"""
    return asyncio.run(analyze_text_with_openai(text, paper_id))


@bot.event
async def on_ready():
    LOGGER.info(f"Bot ready: {bot.user}")
    LOGGER.info(f"Bot intents: {bot.intents}")


@bot.event
async def on_raw_reaction_add(payload: discord.RawReactionActionEvent):
    """Raw ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ©ãƒ å«ã‚€ï¼‰"""
    LOGGER.info(f"=== ON_RAW_REACTION_ADD CALLED ===")
    LOGGER.info(f"Payload: user_id={payload.user_id}, channel_id={payload.channel_id}, message_id={payload.message_id}, emoji={payload.emoji}")
    
    # ãƒœãƒƒãƒˆè‡ªèº«ã®ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ç„¡è¦–
    if payload.user_id == bot.user.id:
        LOGGER.info("Ignoring bot reaction")
        return
    
    # ãƒãƒ£ãƒ³ãƒãƒ«ã¾ãŸã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—
    channel = bot.get_channel(payload.channel_id)
    if channel is None:
        try:
            channel = await bot.fetch_channel(payload.channel_id)
        except Exception as e:
            LOGGER.error(f"Failed to fetch channel: {e}")
            return
    
    # ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã€ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆè¦ªãŒãƒ•ã‚©ãƒ¼ãƒ©ãƒ ï¼‰ã‹ç¢ºèª
    is_forum_post = False
    if hasattr(channel, 'parent_id') and channel.parent_id == DISCORD_FORUM_CHANNEL_ID:
        # ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ©ãƒ æŠ•ç¨¿ï¼‰
        is_forum_post = True
        LOGGER.info(f"âœ… Thread in forum: parent_id={channel.parent_id}")
    elif payload.channel_id == DISCORD_FORUM_CHANNEL_ID:
        # ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãƒãƒ£ãƒ³ãƒãƒ«ç›´ä¸‹ï¼ˆãŸã ã—é€šå¸¸ã¯ã“ãªã„ã¯ãšï¼‰
        is_forum_post = True
        LOGGER.info(f"âœ… Direct forum channel")
    else:
        LOGGER.warning(f"Not in target forum: channel_id={payload.channel_id}, parent_id={getattr(channel, 'parent_id', None)}")
        return
    
    # æŒ‡å®šã•ã‚ŒãŸãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµµæ–‡å­—ã‹ç¢ºèª
    if str(payload.emoji) != REACTION_EMOJI:
        LOGGER.warning(f"Wrong emoji: {str(payload.emoji)} != {REACTION_EMOJI}")
        return
    
    LOGGER.info(f"âœ… All conditions matched! Processing reaction...")
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
    try:
        channel = bot.get_channel(payload.channel_id)
        if channel is None:
            channel = await bot.fetch_channel(payload.channel_id)
        message = await channel.fetch_message(payload.message_id)
    except Exception as e:
        LOGGER.error(f"Failed to fetch message: {e}")
        return
    
    LOGGER.info(f"Message content: {message.content[:100]}")

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰arXivãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    m = ARXIV_PDF_RE.search(message.content)
    if not m:
        LOGGER.warning("No arXiv link found in message")
        return

    arxiv_id = m.group(1)
    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    LOGGER.info(f"Detected arXiv id: {arxiv_id} -> {pdf_url}")

    # åˆ†æç”¨ãƒãƒ£ãƒ³ãƒãƒ«ã‚’å–å¾—
    try:
        analysis_channel = await bot.fetch_channel(DISCORD_ARXIV_ANALYSIS_CHANNEL_ID)
    except Exception as e:
        LOGGER.error(f"Failed to fetch analysis channel: {e}")
        return

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ©ãƒ ãªã®ã§æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆï¼‰
    try:
        # REST API ã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆ
        url = f"https://discord.com/api/v10/channels/{DISCORD_ARXIV_ANALYSIS_CHANNEL_ID}/threads"
        headers = {
            "Authorization": f"Bot {DISCORD_BOT_TOKEN}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "name": f"Analysis: arXiv:{arxiv_id}",
            "message": {
                "content": f"ğŸ” Downloading and analyzing arXiv:{arxiv_id} from {message.author}..."
            }
        }
        
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code not in [200, 201]:
            LOGGER.error(f"Failed to create thread: {response.status_code} - {response.text}")
            return
        
        thread_data = response.json()
        thread_id = thread_data.get('id')
        thread = await bot.fetch_channel(thread_id)
        LOGGER.info(f"âœ“ Thread created: ID={thread_id}")
        
    except Exception as e:
        LOGGER.error(f"Failed to create thread: {e}")
        return

    try:
        r = requests.get(pdf_url, timeout=30)
        r.raise_for_status()
        pdf_bytes = r.content

        text = await extract_text_from_pdf_bytes(pdf_bytes)
        
        # OpenAI API ã‚’ ThreadPoolExecutor ã§éåŒæœŸå®Ÿè¡Œ
        loop = asyncio.get_event_loop()
        japanese_analysis, english_analysis = await loop.run_in_executor(
            executor, 
            lambda: analyze_text_with_openai_sync(text, arxiv_id)
        )

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸1: ã‚ªãƒªã‚¸ãƒŠãƒ«æŠ•ç¨¿æƒ…å ±
        original_info = f"""ğŸ”— **Original Post Information**
**arXiv ID:** {arxiv_id}
**URL:** https://arxiv.org/abs/{arxiv_id}
**Posted by:** {message.author}
**Original message:** [Link]({message.jump_url})
"""
        await thread.send(original_info)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸2: æ—¥æœ¬èªã®åˆ†æï¼ˆé•·ã„å ´åˆã¯åˆ†å‰²ï¼‰
        japanese_header = "ğŸ“– **æ—¥æœ¬èªã§ã®åˆ†æ**\n\n"
        japanese_chunks = split_discord_message(japanese_analysis, max_length=1900)
        for i, chunk in enumerate(japanese_chunks):
            if i == 0:
                msg = japanese_header + chunk
            else:
                msg = f"**(ç¶šã)**\n{chunk}"
            await thread.send(msg)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸3: è‹±èªã®åˆ†æï¼ˆé•·ã„å ´åˆã¯åˆ†å‰²ï¼‰
        english_header = "ğŸ“ **English Analysis**\n\n"
        english_chunks = split_discord_message(english_analysis, max_length=1900)
        for i, chunk in enumerate(english_chunks):
            if i == 0:
                msg = english_header + chunk
            else:
                msg = f"**(Continued)**\n{chunk}"
            await thread.send(msg)
        
        LOGGER.info(f"âœ… Analysis completed and posted for arXiv:{arxiv_id}")

    except Exception as e:
        LOGGER.exception("Failed to analyze PDF")
        try:
            await thread.send(f"âŒ Failed to analyze arXiv:{arxiv_id}: {str(e)[:200]}")
        except Exception:
            pass





if __name__ == '__main__':
    if not DISCORD_BOT_TOKEN:
        print("DISCORD_BOT_TOKEN not set, exiting")
        raise SystemExit(1)

    bot.run(DISCORD_BOT_TOKEN)
