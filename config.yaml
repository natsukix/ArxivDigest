# カテゴリのリストを指定してください（複数のトピックにまたがっても構いません）
# カテゴリの自然言語名を使用してください（https://arxiv.org で確認できます）
# より多くのカテゴリを含めると、大規模言語モデルへの呼び出しが増加します
# 
# 例: ["Artificial Intelligence", "Systems and Control", "Number Theory"]
# この例では、Computer Science、Electrical Engineering、Mathematicsの3つのトピックから自動的に論文を取得します
# //https://arxiv.org/category_taxonomy
# arxiv_category_taxonomy_jp.mdの興味のある英語カテゴリ名を入れる
categories: [
  "Artificial Intelligence",
  "Robotics",
  "Machine Learning",
  "Computer Vision and Pattern Recognition",
  "Audio and Speech Processing",
  "Image and Video Processing",
  "Multiagent Systems"
  ]

# 関連性スコアの閾値。大規模言語モデルからこの値未満のスコアを受け取った論文は
# フィルタリングされます。
#
# 1-10の範囲で設定する必要があります
threshold: 7

# LLMで処理する論文の最大総数（カテゴリ間で均等に分配されます）
max_papers: 12

# LLM評価フェーズで使用するモデル（デフォルト: gpt-4o-mini）
evaluation_model: "gpt-4o-mini"

# 要約生成フェーズで使用するモデル（デフォルト: gpt-3.5-turbo）
summary_model: "gpt-4o-mini"

# 大規模言語モデルがどの論文が関連しているかを判断するために使用する自然言語の記述
#
# 例:
#     "I am interested in complexity theory papers that establish upper bounds"
#     "gas chromatography, mass spectrometry"
#     "making lots of money"
#
# これは空でも構いません。その場合、判断やフィルタリングなしに、arXivが応答する順序で
# 全ての論文リストを返します。
interest: |
  **Priority: Papers achieving state-of-the-art (SotA) results with practical applicability**
  
  **High Priority Areas:**
  1. **Anomaly Sound Detection & Sound Event Detection**
     - Unsupervised and self-supervised anomaly detection methods
     - Industrial sound monitoring and predictive maintenance applications
     - Few-shot and zero-shot anomaly detection approaches
     - Audio event detection and sound event classification (environmental sounds, acoustic scenes)
     - Real-time detection systems with practical deployment
     - Novel feature extraction and representation learning for audio analysis
  
  2. **Large Language Models (LLMs)**
     - Novel pretraining, finetuning, and prompting techniques with empirical improvements
     - Efficient inference methods (quantization, pruning, distillation)
     - Context window extension and long-context understanding
     - Multimodal LLMs (vision-language, audio-language models)
     - Real-world deployment optimizations and cost reduction techniques
  
  3. **Video Understanding**
     - Video-language models and video captioning with SotA results
     - Temporal reasoning and action recognition breakthroughs
     - Efficient video processing architectures
     - Practical video analysis systems with demonstrated real-world impact
  
  4. **Speech Recognition & Audio Processing**
     - End-to-end speech recognition with improved accuracy
     - Multilingual and code-switching speech models
     - Real-time speech processing and streaming ASR
     - Audio-visual speech recognition and multimodal fusion
     - Practical voice interface systems
  
  **General Criteria:**
  - **Prefer papers accepted at major conferences or journals (e.g., NeurIPS, ICML, ICLR, CVPR, ICASSP, IEEE/ACM journals)**
  - Clear performance comparisons with baselines on established benchmarks
  - Reproducible results with code/model availability preferred
  - Practical applicability and deployment feasibility
  - Novel architectural innovations or training methodologies
  
  **Lower Priority:**
  - Papers without peer review or conference/journal acceptance information
  - Language-specific NLP (e.g., Arabic, Chinese-only papers) unless groundbreaking
  - Narrow domain applications without broader impact
  - Incremental improvements (<5% gain) without significant novelty
  - Papers lacking quantitative evaluation or baseline comparisons
